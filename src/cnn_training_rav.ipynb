{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2DwsQiSb6xR"
      },
      "source": [
        "## Oxford AI Summit: Kaggle dataset training notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4Vqn5oayGjh",
        "outputId": "684c8846-97ca-4517-dd2f-96eac3c7afcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in c:\\users\\konrad\\documents\\projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages (1.6.14)\n",
            "Requirement already satisfied: python-slugify in c:\\users\\konrad\\documents\\projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in c:\\users\\konrad\\documents\\projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages (from kaggle) (2.2.1)\n",
            "Requirement already satisfied: six>=1.10 in c:\\users\\konrad\\documents\\projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: bleach in c:\\users\\konrad\\documents\\projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: requests in c:\\users\\konrad\\documents\\projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\konrad\\documents\\projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\konrad\\documents\\projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\konrad\\documents\\projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\konrad\\documents\\projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: webencodings in c:\\users\\konrad\\documents\\projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\konrad\\documents\\projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\konrad\\documents\\projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages (from requests->kaggle) (3.7)\n",
            "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\konrad\\documents\\projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
            "You should consider upgrading via the 'c:\\Users\\Konrad\\Documents\\Projects\\oxford-ai-summit-hackathon-group-4\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "%pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pk4SACVlb6xT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzngwmYNx6NO",
        "outputId": "68447921-6110-4757-cf8c-7b1863d13ea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on GPU or CPU\n"
          ]
        }
      ],
      "source": [
        "# Enable TPU if available\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    print('Running on TPU')\n",
        "except ValueError:\n",
        "    strategy = tf.distribute.get_strategy()  # Default strategy for CPU and GPU\n",
        "    print('Running on GPU or CPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8ye6k36lXxA",
        "outputId": "e5e8206c-44e3-4a73-8ed5-9e146139a270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-dataset\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mparamaggarwal/fashion-product-images-dataset\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      4\u001b[0m destination_folder \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfashion_product_images\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m api\u001b[39m.\u001b[39;49mdataset_download_files(dataset, path\u001b[39m=\u001b[39;49mdestination_folder, unzip\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[1;32mc:\\Users\\Konrad\\Documents\\Projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages\\kaggle\\api\\kaggle_api_extended.py:1499\u001b[0m, in \u001b[0;36mKaggleApi.dataset_download_files\u001b[1;34m(self, dataset, path, force, quiet, unzip, licenses)\u001b[0m\n\u001b[0;32m   1497\u001b[0m outfile \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(effective_path, dataset_slug \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.zip\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m force \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_needed(response, outfile, quiet):\n\u001b[1;32m-> 1499\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_file(response, outfile, quiet, \u001b[39mnot\u001b[39;49;00m force)\n\u001b[0;32m   1500\u001b[0m     downloaded \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Konrad\\Documents\\Projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages\\kaggle\\api\\kaggle_api_extended.py:1958\u001b[0m, in \u001b[0;36mKaggleApi.download_file\u001b[1;34m(self, response, outfile, quiet, resume, chunk_size)\u001b[0m\n\u001b[0;32m   1956\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(outfile, open_mode) \u001b[39mas\u001b[39;00m out:\n\u001b[0;32m   1957\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1958\u001b[0m         data \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39;49mread(chunk_size)\n\u001b[0;32m   1959\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[0;32m   1960\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Konrad\\Documents\\Projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages\\urllib3\\response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[0;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[1;32m--> 935\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[0;32m    937\u001b[0m flush_decoder \u001b[39m=\u001b[39m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m (amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data)\n\u001b[0;32m    939\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Konrad\\Documents\\Projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages\\urllib3\\response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    859\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    861\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 862\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt, read1\u001b[39m=\u001b[39;49mread1) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    863\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    871\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
            "File \u001b[1;32mc:\\Users\\Konrad\\Documents\\Projects\\oxford-ai-summit-hackathon-group-4\\venv\\lib\\site-packages\\urllib3\\response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread1(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread1()\n\u001b[0;32m    843\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    844\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 845\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:458\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    456\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    457\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 458\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    460\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:502\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    497\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    499\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    504\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    505\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    506\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "api = KaggleApi()\n",
        "\n",
        "dataset = 'paramaggarwal/fashion-product-images-dataset'\n",
        "destination_folder = 'fashion_product_images'\n",
        "\n",
        "api.dataset_download_files(dataset, path=destination_folder, unzip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4j8ztQcqkYH",
        "outputId": "bc4e8333-0783-4cd5-d548-4c39b98f421d"
      },
      "outputs": [],
      "source": [
        "# Load the metadata\n",
        "metadata_path = 'fashion_product_images/fashion-dataset/styles.csv'\n",
        "metadata = pd.read_csv(metadata_path, on_bad_lines='skip')\n",
        "\n",
        "# Display the first few rows of the metadata\n",
        "print(metadata.head())\n",
        "print(metadata.columns)\n",
        "\n",
        "# Replace 'id' with the correct column name containing the unique identifier\n",
        "image_folder = 'fashion_product_images/fashion-dataset/images'\n",
        "metadata['image_path'] = metadata.apply(lambda row: os.path.join(image_folder, str(row['id']) + '.jpg'), axis=1)\n",
        "metadata = metadata[metadata['image_path'].apply(os.path.exists)]\n",
        "\n",
        "# Select relevant columns and encode labels\n",
        "metadata = metadata[['image_path', 'articleType']]\n",
        "metadata['articleType'] = metadata['articleType'].astype('category')\n",
        "metadata['label'] = metadata['articleType'].cat.codes\n",
        "\n",
        "# Ensure each class has at least 2 samples\n",
        "min_samples_per_class = 2\n",
        "class_counts = metadata['label'].value_counts()\n",
        "valid_classes = class_counts[class_counts >= min_samples_per_class].index\n",
        "metadata = metadata[metadata['label'].isin(valid_classes)]\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_df, val_df = train_test_split(metadata, test_size=0.2, stratify=metadata['label'], random_state=5)\n",
        "\n",
        "# Convert the labels to strings\n",
        "train_df['label'] = train_df['label'].astype(str)\n",
        "val_df['label'] = val_df['label'].astype(str)\n",
        "\n",
        "# Find common classes\n",
        "train_classes = set(train_df['label'].unique())\n",
        "val_classes = set(val_df['label'].unique())\n",
        "print(f\"Len: train({len(train_classes)}), val({len(val_classes)})\")\n",
        "print(train_classes - val_classes)\n",
        "common_classes = train_classes.intersection(val_classes)\n",
        "\n",
        "# Filter dataframes to only include common classes\n",
        "train_df = train_df[train_df['label'].isin(common_classes)]\n",
        "val_df = val_df[val_df['label'].isin(common_classes)]\n",
        "\n",
        "# Print the number of unique labels\n",
        "num_classes = len(common_classes)\n",
        "print(f'Number of unique labels: {num_classes}')\n",
        "print(f'Training set size: {len(train_df)}')\n",
        "print(f'Validation set size: {len(val_df)}')\n",
        "\n",
        "print(f'Training set size: {len(train_df)}')\n",
        "print(f'Validation set size: {len(val_df)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9wdwGfSsclI",
        "outputId": "1d1f6ff2-9856-445e-b0a2-71ad72290c1a"
      },
      "outputs": [],
      "source": [
        "# Image data generator with augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Image data generator for validation (without augmentation)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Data generators\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "    val_df,\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKaZJBXMb6xV",
        "outputId": "da10066c-d2b7-4fb7-f148-798e6501a35e"
      },
      "outputs": [],
      "source": [
        "# Define the CNN model within the strategy scope\n",
        "with strategy.scope():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')  # Adjusted number of output units\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw-fLn0Mb6xV",
        "outputId": "2c034cf5-4db1-4f81-c7e3-c994b67aa7cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.EarlyStopping at 0x208d51268e0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "EarlyStopping(\n",
        "    monitor=\"categorical_crossentropy\",\n",
        "    min_delta=0,\n",
        "    patience=3,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False,\n",
        "    start_from_epoch=0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(\u001b[39m'\u001b[39m\u001b[39mfashion_mnist_model.keras\u001b[39m\u001b[39m'\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(\n\u001b[0;32m      4\u001b[0m     train_generator,\n\u001b[0;32m      5\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[0;32m      6\u001b[0m     validation_data\u001b[39m=\u001b[39mval_generator,\n\u001b[0;32m      7\u001b[0m     callbacks\u001b[39m=\u001b[39m[checkpoint],\n\u001b[0;32m      8\u001b[0m )\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train the model\n",
        "checkpoint = ModelCheckpoint('fashion_mnist_model.keras', save_best_only=True)\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[checkpoint],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FVF0p62u_Ak"
      },
      "outputs": [],
      "source": [
        "# Basic evaluation\n",
        "val_loss, val_acc = model.evaluate(val_generator)\n",
        "print(f'Validation loss: {val_loss}')\n",
        "print(f'Validation accuracy: {val_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0PbKbMfb6xV"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save('fashion_mnist_model_final.keras')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
